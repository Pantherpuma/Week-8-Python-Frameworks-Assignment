{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbd579b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfabda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data file (use full metadata.csv if available, otherwise fallback to sample)\n",
    "base = os.path.join(os.getcwd(), '..')  # notebook sits inside /notebooks\n",
    "paths = [\n",
    "    os.path.join(base, 'data', 'metadata.csv'),\n",
    "    os.path.join(base, 'data', 'metadata_sample.csv')\n",
    "]\n",
    "data_path = None\n",
    "for p in paths:\n",
    "    if os.path.exists(p):\n",
    "        data_path = p\n",
    "        break\n",
    "if data_path is None:\n",
    "    raise FileNotFoundError('No metadata file found in data/. Place metadata.csv or metadata_sample.csv in the data/ folder')\n",
    "print('Loading', data_path)\n",
    "df = pd.read_csv(data_path, low_memory=False)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d312b463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick preview and info\n",
    "display(df.head())\n",
    "print('\n",
    "Dataframe info:')\n",
    "print(df.dtypes)\n",
    "print('\n",
    "Missing value counts for key columns:')\n",
    "print(df[['title','abstract','publish_time','journal','source_x']].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e62e6be",
   "metadata": {},
   "source": [
    "## Cleaning and feature engineering\n",
    "We'll convert the publish time into datetime, extract a `year` column and add an `abstract_word_count`. We'll keep only rows where we have at least a title or abstract for analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c56b48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dates and add year column\n",
    "df['publish_time'] = pd.to_datetime(df['publish_time'], errors='coerce')\n",
    "df['year'] = df['publish_time'].dt.year\n",
    "# Add abstract word count (fill missing abstract with empty string first)\n",
    "df['abstract'] = df['abstract'].fillna('')\n",
    "df['abstract_word_count'] = df['abstract'].str.split().apply(len)\n",
    "# Basic filtering: keep rows that have a title or abstract text\n",
    "df_clean = df[(df['title'].notna()) | (df['abstract_word_count'] > 0)].copy()\n",
    "print('Before:', len(df), 'After cleaning:', len(df_clean))\n",
    "df_clean[['title','abstract','publish_time','year','abstract_word_count']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c271fd",
   "metadata": {},
   "source": [
    "### Publications by year\n",
    "Count papers by publication year and visualize a simple bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f7f82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_counts = df_clean['year'].value_counts().sort_index()\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.barplot(x=year_counts.index.astype('Int64'), y=year_counts.values, palette='Blues_d')\n",
    "plt.title('Publications by Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of papers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6927fe",
   "metadata": {},
   "source": [
    "### Top publishing journals\n",
    "Show the most frequent journals in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7cd18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_journals = df_clean['journal'].value_counts().nlargest(10)\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(y=top_journals.index, x=top_journals.values, palette='viridis')\n",
    "plt.title('Top journals by paper count')\n",
    "plt.xlabel('Number of papers')\n",
    "plt.ylabel('Journal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68533de",
   "metadata": {},
   "source": [
    "### Most frequent words in titles\n",
    "We’ll do a simple tokenization, remove stop words and punctuation, and show a frequency table and word cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b5fe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def simple_tokens(text):\n",
    "    text = (text or '').lower()\n",
    "    text = re.sub(r'[^a-z]', ' ', text)\n",
    "    tokens = [t for t in text.split() if t and t not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "all_title_tokens = []\n",
    "for t in df_clean['title'].fillna(''):\n",
    "    all_title_tokens += simple_tokens(t)\n",
    "cnt = Counter(all_title_tokens)\n",
    "most_common = cnt.most_common(30)\n",
    "pd.DataFrame(most_common, columns=['word','count']).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ae9c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word cloud for titles\n",
    "wc = WordCloud(width=900, height=400, background_color='white', stopwords=stop_words).generate(' '.join(all_title_tokens))\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Wordcloud of title words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde8a315",
   "metadata": {},
   "source": [
    "### Distribution of papers by source (source_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a28995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_counts = df_clean['source_x'].value_counts()\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(x=src_counts.index, y=src_counts.values, palette='magma')\n",
    "plt.title('Paper counts by source')\n",
    "plt.xlabel('Source')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdea944",
   "metadata": {},
   "source": [
    "### Wrap up / next steps\n",
    "This notebook demonstrates the analysis pipeline requested in the assignment. For the full dataset you may: explore abstracts with TF-IDF, do topic modeling, include more interactive plots, or add filtering by author or journal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e687228",
   "metadata": {},
   "source": [
    "## TF-IDF and topic modeling (starter)\n",
    "\n",
    "Below are example steps you can use to vectorize titles/abstracts with TF-IDF and run a lightweight LDA topic model on the collection. This is intentionally simple so it will run quickly on the sample dataset; for the full `metadata.csv` you may want to sample or increase compute resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bab4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports for TF-IDF and LDA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Prepare a corpus - prefer abstract if available, otherwise title\n",
    "corpus = df_clean['abstract'].fillna('').replace('', pd.NA).fillna(df_clean['title'].fillna(''))\n",
    "corpus = corpus.astype(str).str.strip()\n",
    "corpus = corpus[corpus.str.len() > 0]\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330ab3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF — show top features for the corpus (small demo)\n",
    "tfidf = TfidfVectorizer(max_df=0.9, min_df=1, max_features=1000, stop_words='english')\n",
    "X = tfidf.fit_transform(corpus)\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "# Show top features by average tf-idf across docs\n",
    "import numpy as np\n",
    "avg_tfidf = np.asarray(X.mean(axis=0)).ravel()\n",
    "top_idx = avg_tfidf.argsort()[::-1][:30]\n",
    "pd.DataFrame({'term': feature_names[top_idx], 'avg_tfidf': avg_tfidf[top_idx]}).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910a67fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA topic modeling using CountVectorizer (small demo with 3 topics)\n",
    "n_topics = 3\n",
    "cv = CountVectorizer(max_df=0.95, min_df=1, max_features=1000, stop_words='english')\n",
    "X_counts = cv.fit_transform(corpus)\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42, learning_method='batch')\n",
    "lda.fit(X_counts)\n",
    "\n",
    "# show top words per topic\n",
    "def print_top_words(model, feature_names, n_top_words=8):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[::-1][:n_top_words]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        print(f'Topic {topic_idx}:', ' '.join(top_features))\n",
    "\n",
    "print_top_words(lda, cv.get_feature_names_out(), n_top_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e457a9b1",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "- This is a minimal TF-IDF + LDA workflow for demonstration. For the full dataset: tune `min_df`, `max_df`, `n_components`, and consider using larger vocabulary, n-grams, and more robust preprocessing.\n",
    "- For larger corpora, prefer incremental or online LDA, or use libraries like gensim for scalability."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
